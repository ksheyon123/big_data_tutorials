# ì§„ì§œ ë¹…ë°ì´í„° ì˜ˆì œ - ë©”ëª¨ë¦¬ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” ë°ì´í„° ì²˜ë¦¬

import pandas as pd
import numpy as np
from dask import dataframe as dd
import time

print("=== ì¼ë°˜ pandas vs ë¹…ë°ì´í„° ì²˜ë¦¬ ë¹„êµ ===\n")

# 1. ë©”ëª¨ë¦¬ í•œê³„ ì‹œë®¬ë ˆì´ì…˜
print("1. ë©”ëª¨ë¦¬ í•œê³„ ìƒí™© ë§Œë“¤ê¸°")

def create_large_dataset():
    """
    ì‹¤ì œë¡œ ë©”ëª¨ë¦¬ í•œê³„ì— ë¶€ë”ªíˆëŠ” ìƒí™©ì„ ì‹œë®¬ë ˆì´ì…˜
    """
    print("ëŒ€ìš©ëŸ‰ ê°€ìƒ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # 1ì–µ í–‰ì˜ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” íŒŒì¼ë“¤ë¡œ ë¶„ì‚° ì €ì¥ë¨)
    # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ëŸ° í¬ê¸°ì˜ ë°ì´í„°ê°€ ì¼ë°˜ì 
    
    chunk_size = 1000000  # 100ë§Œ í–‰ì”© ì²˜ë¦¬
    total_rows = 10000000  # 1000ë§Œ í–‰ (ì‹¤ì œ ë¹…ë°ì´í„°ëŠ” ìˆ˜ì‹­ì–µ í–‰)
    
    print(f"ì´ {total_rows:,}í–‰ ë°ì´í„° ì²˜ë¦¬ ì˜ˆì •")
    print(f"ì²­í¬ í¬ê¸°: {chunk_size:,}í–‰")
    
    return chunk_size, total_rows

chunk_size, total_rows = create_large_dataset()

print("\n" + "="*60)
print("2. ì¼ë°˜ pandas ë°©ì‹ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜)")

def pandas_approach_simulation():
    """
    ì¼ë°˜ pandasë¡œëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ìƒí™©
    """
    try:
        print("âŒ pandas.read_csv('huge_file.csv')  # ë©”ëª¨ë¦¬ ë¶€ì¡±!")
        print("âŒ MemoryError: Unable to allocate 8.5 GiB for an array")
        print("âŒ ì¼ë°˜ì ì¸ ì»´í“¨í„°ë¡œëŠ” ì²˜ë¦¬ ë¶ˆê°€ëŠ¥")
        return False
    except:
        return False

pandas_approach_simulation()

print("\n" + "="*60)
print("3. ë¹…ë°ì´í„° ë°©ì‹ - ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬")

def bigdata_chunk_processing():
    """
    ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ (MapReduce ê°œë…)
    """
    results = []
    
    print("ğŸ”„ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„ì‚° ì²˜ë¦¬ ì‹œì‘...")
    
    for chunk_num in range(0, total_rows, chunk_size):
        # ê° ì²­í¬ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
        end_row = min(chunk_num + chunk_size, total_rows)
        
        # ê°€ìƒ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” íŒŒì¼ì—ì„œ ì½ìŒ)
        chunk_data = {
            'user_id': np.random.randint(1, 1000000, chunk_size),
            'timestamp': pd.date_range('2024-01-01', periods=chunk_size, freq='1s'),
            'page_views': np.random.randint(1, 100, chunk_size),
            'session_duration': np.random.randint(10, 3600, chunk_size)
        }
        
        df_chunk = pd.DataFrame(chunk_data)
        
        # ê° ì²­í¬ì—ì„œ ì§‘ê³„ ì²˜ë¦¬
        chunk_result = {
            'chunk': chunk_num // chunk_size + 1,
            'total_users': df_chunk['user_id'].nunique(),
            'avg_page_views': df_chunk['page_views'].mean(),
            'total_sessions': len(df_chunk)
        }
        
        results.append(chunk_result)
        
        if chunk_num < 3000000:  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ ì¶œë ¥
            print(f"  âœ… ì²­í¬ {chunk_result['chunk']} ì²˜ë¦¬ ì™„ë£Œ - "
                  f"ì‚¬ìš©ì {chunk_result['total_users']:,}ëª…, "
                  f"í‰ê·  í˜ì´ì§€ë·° {chunk_result['avg_page_views']:.1f}")
    
    return results

chunk_results = bigdata_chunk_processing()

print(f"\nğŸ¯ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(chunk_results)}ê°œ ì²­í¬")

print("\n" + "="*60)
print("4. ì‹¤ì œ ë¹…ë°ì´í„° ë„êµ¬ - Dask ì‚¬ìš© ì˜ˆì œ")

def dask_bigdata_example():
    """
    Dask: pandasì˜ ë¹…ë°ì´í„° ë²„ì „
    """
    print("ğŸš€ Daskë¡œ ë¶„ì‚° ì²˜ë¦¬ (pandas APIì™€ ë™ì¼í•˜ì§€ë§Œ ë¶„ì‚° ì²˜ë¦¬)")
    
    try:
        # ê°€ìƒì˜ ëŒ€ìš©ëŸ‰ íŒŒì¼ë“¤ (ì‹¤ì œë¡œëŠ” ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„ì‚° ì €ì¥)
        print("# ì‹¤ì œ ì½”ë“œ ì˜ˆì œ:")
        print("dd.read_csv('data/*.csv')  # í´ë”ì˜ ëª¨ë“  CSVë¥¼ ë¶„ì‚° ì²˜ë¦¬")
        print("dd.read_parquet('data/*.parquet')  # í˜íƒ€ë°”ì´íŠ¸ê¸‰ ë°ì´í„°ë„ ì²˜ë¦¬")
        
        # ê°„ë‹¨í•œ dask ì˜ˆì œ
        # í° ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì§€ì—° ì—°ì‚°)
        df_large = dd.from_pandas(
            pd.DataFrame({
                'x': np.random.randn(1000000),
                'y': np.random.randn(1000000)
            }), 
            npartitions=4  # 4ê°œ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„ì‚°
        )
        
        print(f"\nğŸ“Š Dask DataFrame ìƒì„±:")
        print(f"   - íŒŒí‹°ì…˜ ìˆ˜: {df_large.npartitions}")
        print(f"   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì§€ì—° ì—°ì‚°ìœ¼ë¡œ ìµœì†Œí™”")
        
        # ì—°ì‚° (ì§€ì—° ì‹¤í–‰)
        result = df_large.x.mean()
        print(f"   - í‰ê· ê°’ ê³„ì‚°: {result.compute():.4f}")
        
        return True
        
    except Exception as e:
        print(f"Dask ì˜ˆì œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("pip install daskë¡œ ì„¤ì¹˜ í›„ ì¬ì‹œë„í•´ë³´ì„¸ìš”")
        return False

dask_success = dask_bigdata_example()

print("\n" + "="*60)
print("5. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")

def stream_processing_simulation():
    """
    ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„° ì²˜ë¦¬ (Kafka + Spark Streaming ê°œë…)
    """
    print("ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
    for second in range(5):
        # 1ì´ˆë§ˆë‹¤ ìˆ˜ì²œ ê±´ì˜ ë°ì´í„°ê°€ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •
        events_per_second = np.random.randint(1000, 5000)
        
        # ì‹¤ì‹œê°„ ì§‘ê³„
        print(f"â° {second+1}ì´ˆ: {events_per_second:,}ê°œ ì´ë²¤íŠ¸ ì²˜ë¦¬")
        print(f"   - ì‹¤ì‹œê°„ ë¶„ì„: í‰ê·  ì‘ë‹µì‹œê°„, ì—ëŸ¬ìœ¨, íŠ¸ë˜í”½ íŒ¨í„´")
        
        time.sleep(0.5)  # ì‹¤ì œë¡œëŠ” ì‹¤ì‹œê°„ ì²˜ë¦¬
    
    print("âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ ì™„ë£Œ")

stream_processing_simulation()

print("\n" + "="*60)
print("ğŸ¯ ë¹…ë°ì´í„° vs ì¼ë°˜ ë°ì´í„° ë¶„ì„ ì°¨ì´ì  ì •ë¦¬")

comparison = """
ì¼ë°˜ ë°ì´í„° ë¶„ì„:
âŒ pandas.read_csv() - ë©”ëª¨ë¦¬ì— ì „ì²´ ë¡œë“œ
âŒ ë‹¨ì¼ ë¨¸ì‹  ì²˜ë¦¬
âŒ ë°°ì¹˜ ì²˜ë¦¬ë§Œ ê°€ëŠ¥
âŒ êµ¬ì¡°í™”ëœ ë°ì´í„°ë§Œ

ë¹…ë°ì´í„° ì²˜ë¦¬:
âœ… ì²­í¬/íŒŒí‹°ì…˜ ë‹¨ìœ„ ë¶„ì‚° ì²˜ë¦¬
âœ… ì—¬ëŸ¬ ë¨¸ì‹ ì—ì„œ ë³‘ë ¬ ì²˜ë¦¬ (í´ëŸ¬ìŠ¤í„°)
âœ… ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ê°€ëŠ¥
âœ… ë‹¤ì–‘í•œ í˜•íƒœ ë°ì´í„° (í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì„¼ì„œ...)
âœ… í˜íƒ€ë°”ì´íŠ¸ê¸‰ ë°ì´í„° ì²˜ë¦¬ ê°€ëŠ¥

ì‚¬ìš© ë„êµ¬:
ğŸ”§ Apache Spark (PySpark)
ğŸ”§ Dask (Python)
ğŸ”§ Apache Kafka (ìŠ¤íŠ¸ë¦¬ë°)
ğŸ”§ Hadoop (ë¶„ì‚° ì €ì¥)
ğŸ”§ AWS EMR, Google Dataproc (í´ë¼ìš°ë“œ)
"""

print(comparison)

print("\nğŸ’¡ ë‹¤ìŒ í•™ìŠµ ë‹¨ê³„:")
print("1. Dockerë¡œ Spark í™˜ê²½ êµ¬ì¶•")
print("2. ì‹¤ì œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ìœ¼ë¡œ PySpark ì‹¤ìŠµ")
print("3. AWS/GCPì—ì„œ í´ëŸ¬ìŠ¤í„° êµ¬ì„±")
print("4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")