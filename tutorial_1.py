# ë¹…ë°ì´í„° ì…ë¬¸ - ì‹¤ì œ ë°ì´í„°ë¡œ ì‹œì‘í•˜ê¸°
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import requests

# 1. COVID-19 ë°ì´í„° ë¶„ì„ (ì‹¤ì‹œê°„ ë°ì´í„°)
print("=== COVID-19 ë°ì´í„° ë¶„ì„ ===")
covid_url = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"

try:
    covid_df = pd.read_csv(covid_url)
    print(f"ë°ì´í„° í¬ê¸°: {covid_df.shape}")
    print("ìƒìœ„ 5ê°œ êµ­ê°€:")
    
    # ìµœì‹  ë°ì´í„° ì»¬ëŸ¼ (ë‚ ì§œ)
    latest_col = covid_df.columns[-1]
    top_countries = covid_df.groupby('Country/Region')[latest_col].sum().sort_values(ascending=False).head()
    print(top_countries)
    
except Exception as e:
    print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

print("\n" + "="*50 + "\n")

# 2. ì£¼ì‹ ë°ì´í„° ë¶„ì„ (yfinance í•„ìš”: pip install yfinance)
print("=== ì£¼ì‹ ë°ì´í„° ë¶„ì„ ===")
try:
    import yfinance as yf
    
    # ì‚¼ì„±ì „ì, ì• í”Œ ë°ì´í„°
    tickers = ['005930.KS', 'AAPL']  # ì‚¼ì„±ì „ì, ì• í”Œ
    stock_data = {}
    
    for ticker in tickers:
        stock = yf.Ticker(ticker)
        stock_data[ticker] = stock.history(period="3mo")
    
    # ìˆ˜ìµë¥  ê³„ì‚°
    for ticker, data in stock_data.items():
        returns = data['Close'].pct_change()
        print(f"{ticker} - 3ê°œì›” í‰ê·  ìˆ˜ìµë¥ : {returns.mean():.4f}")
        print(f"{ticker} - ë³€ë™ì„± (í‘œì¤€í¸ì°¨): {returns.std():.4f}")
    
except ImportError:
    print("yfinance ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install yfinance")
except Exception as e:
    print(f"ì£¼ì‹ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

print("\n" + "="*50 + "\n")

# 3. Reddit API ë°ì´í„° (ë¬´ë£Œ)
print("=== Reddit ë°ì´í„° ë¶„ì„ ===")
def get_reddit_data(subreddit, limit=10):
    url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}"
    headers = {'User-Agent': 'DataAnalysis/1.0'}
    
    try:
        response = requests.get(url, headers=headers)
        data = response.json()
        
        posts = []
        for post in data['data']['children']:
            posts.append({
                'title': post['data']['title'][:50] + "...",  # ì œëª© ì¶•ì•½
                'score': post['data']['score'],
                'comments': post['data']['num_comments'],
                'created': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M')
            })
        
        return pd.DataFrame(posts)
    
    except Exception as e:
        print(f"Reddit ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# í”„ë¡œê·¸ë˜ë° ê´€ë ¨ ì„œë¸Œë ˆë”§ ë¶„ì„
reddit_df = get_reddit_data('programming', 5)
if not reddit_df.empty:
    print("Reddit /r/programming ì¸ê¸° í¬ìŠ¤íŠ¸:")
    print(reddit_df.to_string(index=False))
    print(f"\ní‰ê·  ì ìˆ˜: {reddit_df['score'].mean():.1f}")
    print(f"í‰ê·  ëŒ“ê¸€ ìˆ˜: {reddit_df['comments'].mean():.1f}")

print("\n" + "="*50 + "\n")

# 4. ê°„ë‹¨í•œ ë°ì´í„° ìƒì„± ë° ë¶„ì„ (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©)
print("=== ê°€ìƒ ì›¹ ë¡œê·¸ ë°ì´í„° ë¶„ì„ ===")
import random
from datetime import datetime, timedelta

# ê°€ìƒ ì›¹ ë¡œê·¸ ìƒì„±
def generate_web_logs(num_logs=1000):
    pages = ['/home', '/products', '/cart', '/checkout', '/profile', '/about']
    devices = ['mobile', 'desktop', 'tablet']
    countries = ['KR', 'US', 'JP', 'CN', 'DE']
    
    logs = []
    base_time = datetime.now() - timedelta(days=30)
    
    for i in range(num_logs):
        log = {
            'timestamp': base_time + timedelta(
                days=random.randint(0, 30),
                hours=random.randint(0, 23),
                minutes=random.randint(0, 59)
            ),
            'user_id': f'user_{random.randint(1, 200)}',
            'page': random.choice(pages),
            'duration_seconds': random.randint(10, 600),
            'device': random.choice(devices),
            'country': random.choice(countries),
            'referrer': random.choice(['google', 'direct', 'facebook', 'twitter', 'other'])
        }
        logs.append(log)
    
    return pd.DataFrame(logs)

# ì›¹ ë¡œê·¸ ë¶„ì„
web_logs = generate_web_logs(5000)
print(f"ìƒì„±ëœ ë¡œê·¸ ìˆ˜: {len(web_logs)}")

# ë¶„ì„ ì˜ˆì œë“¤
print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
print("1. ê°€ì¥ ì¸ê¸°ìˆëŠ” í˜ì´ì§€:")
popular_pages = web_logs['page'].value_counts().head()
print(popular_pages)

print("\n2. ë””ë°”ì´ìŠ¤ë³„ í‰ê·  ì²´ë¥˜ì‹œê°„:")
device_duration = web_logs.groupby('device')['duration_seconds'].mean().sort_values(ascending=False)
print(device_duration)

print("\n3. êµ­ê°€ë³„ ë°©ë¬¸ì ìˆ˜:")
country_visitors = web_logs['country'].value_counts()
print(country_visitors)

print("\n4. ì‹œê°„ëŒ€ë³„ íŠ¸ë˜í”½ (ìƒìœ„ 5ê°œ):")
web_logs['hour'] = web_logs['timestamp'].dt.hour
hourly_traffic = web_logs['hour'].value_counts().sort_index().head()
print(hourly_traffic)

print("\nâœ… ë¹…ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")
print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì´ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì„œ ë” ë³µì¡í•œ ë¶„ì„ì„ í•´ë³´ì„¸ìš”!")
print("   - ì‹œê°í™” ì¶”ê°€ (matplotlib, seaborn)")
print("   - ì‹¤ì œ API ë°ì´í„° ì—°ë™")
print("   - CSV íŒŒì¼ë¡œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°")
print("   - Pandas ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©")